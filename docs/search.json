[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "paRticles: Quick, unpolished thoughts, discoveries, and ideas about all things R."
  },
  {
    "objectID": "posts/2025-08-12-user-2025/index.html",
    "href": "posts/2025-08-12-user-2025/index.html",
    "title": "UseR!2025: My Top 10",
    "section": "",
    "text": "I’m finally decompressing from another wonderful and whirlwind UseR! conference, so I thought I’d share some quick links with everyone!\nFWIW, I spammed my thoughts throughout the conference over at BlueSky… sorry not sorry to my followers, I may have gone a bit overboard.\nBut, in the interest of a more persistent summary, I figured it might be worth collecting some highlights here!\nNote that for every 4 talks I went to, there were 12 others that overlapped and so I couldn’t. I know for a fact that I missed some super cool stuff. Also, everything I saw was extremely high quality, shoutout to all the speakers for their incredible projects and well-crafted presentations.\nSo, don’t take the following as the “best” stuff, just what stands out to me right now:"
  },
  {
    "objectID": "posts/2025-08-12-user-2025/index.html#top-10-things-im-hyped-about-from-user2025",
    "href": "posts/2025-08-12-user-2025/index.html#top-10-things-im-hyped-about-from-user2025",
    "title": "UseR!2025: My Top 10",
    "section": "Top 10 Things I’m Hyped About From User!2025:",
    "text": "Top 10 Things I’m Hyped About From User!2025:\n\n10. Getting to share my students’ work on tidyclust with y’all.\nThis past year, for the first time, I got to advise some fabulous M.S. students in my department. Three of them ended up working on contributing new algorithms to tidyclust, which was quite an adventure!\n\nMixture models and DBSCAN(, by Brendan Callendar\nFrequent Itemset Mining by Andrew Kerr\nBIRCH, by Jacob Perez\n\nBased on our work, I put together some thoughts in my presentation about what algorithmic and UI decisions need to be made before a new method can be build into the tidyclust (or tidymodels) ecosystem.\n\n\n9. ggplot storytelling using tricks from McCall Pitcher\nMy favorite session at any conference is often the lightning talks! I love seeing the quick little “infomercials” of packages or code that solve small interesting problems.\nIn this one, McCall shared how she modified a plain ggplot to create a very lovely storytelling process for the data.\n\nI’d love to have a wrapper package on this approach that streamlines/generalizes her awesome design!\n\n\n8. R as a “Conductor” of tools, from Simon Urbanek’s Keynote\nThere was a lot of really interesting, meaty content in Simon’s Keynote, but what stuck out to me most was his assertion that R doesn’t need to be best at every task to be the best tool.\nR is and will always be a data-centered language; so it just feels better for working with data. But that doesn’t mean you can’t use other language tools for, e.g. deep learning. The open-source-ness of R means that almost anything you can imagine has a bridge package so you can call upon other tools from within R.\nTo paraphrase the quote as best I can from the notes I jotted down…\n\n“I believe R is an orchestrator. It’s the language that binds your data to all the other tools.”\n\n\n\n\nCute but confusing illustration by ChatGPT\n\n\n\n\n7. hexsession by Luis Verde Arregoitia\nThis is one of those fun little packages that does only one thing, and it’s a needed thing, and it does it well.\nIn this case, hexsession creates nice css hoverable display out of hex stickers, e.g.:\n\nhexsession::make_tile(packages=c(\"dtplyr\", \"dbplyr\", \"duckplyr\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n  \n    created with hexsession\n  \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n6. medley (and clav) by Jason Bryer\nI picked this for two reasons:\nFirst, that I’m excited medley exists. I don’t think we talk enough about missing data - my students always ask me for more content on it! - and I think we focus way too much on interpolation when we do. The idea of splitting your dataset into subsets by patterns of missingness is lovely and (IMHO) should be the norm unless you have a really defensible reason to interpolate a certain way.\nSecond, because after this session, Jason and I got to have a super fun nerd whiteboard chat about validating clusters! His package clav implements some common cluster validation metrics, and we both are very interested in how resampling approaches can help us decide on a “best” clustering.\n\n\n5. tinytable by Vincent Arel-Bundock and tinyplot by Grant McDermott\nDo you remember those ads that said\n\n“There are some things money can’t buy. For everything else, there’s MasterCard.”\n\nThis is how I feel about these two packages. There are some things that need the most developed package for that specific task. For everything else, there’s the tinies.\nFor making professional, complex, camera-ready tables in HTML, I personally believe gt is the best tool. For any other table-making task (e.g. pdf tables, or quick ones for your own exploration), I believe I’ll be swapping to tinytable. It’s somewhat quicker syntax to achieve similar tasks, like conditional cell highlighting, and I love that it has options for docx and pdf output!\n\nlibrary(tinytable)\nx &lt;- mtcars[1:4, 1:5]\ntt(x) |&gt;\n  print(\"markdown\")\n\n\n+------+-----+------+-----+------+\n| mpg  | cyl | disp | hp  | drat |\n+======+=====+======+=====+======+\n| 21.0 | 6   | 160  | 110 | 3.90 |\n+------+-----+------+-----+------+\n| 21.0 | 6   | 160  | 110 | 3.90 |\n+------+-----+------+-----+------+\n| 22.8 | 4   | 108  | 93  | 3.85 |\n+------+-----+------+-----+------+\n| 21.4 | 6   | 258  | 110 | 3.08 |\n+------+-----+------+-----+------+ \n\n\nSimilarly, ggplot is king for visualization and I doubt anyone will argue much with me on that… but I admit I still pull out base graphics when I need to quickly look at something, and many tidyverse devotees I know still might use base in intro classes to avoid too much abstraction. To me tinyplot is the better option in those moments - it provides the simplicity and shortcuts of base graphics but with much nicer (and often ggplot-like) appearance.\n\n\n4. parsermd by Colin Rundel\nThis package (pronounced “parser-md” not “parse-Rmd”) has existed for a while, but is now being released with Quarto support.\nBasically, it’s a way to parse through the source text of your .qmd document and get back an easy reference to each element.\n\nlibrary(parsermd)\n\nparse_qmd(\"index.qmd\") |&gt;\n  as_tibble() |&gt;\n  head() |&gt; \n  as_ast()\n\n├── YAML [7 fields]\n├── Markdown [11 lines]\n└── Heading [h2] - Top 10 Things I'm Hyped About From User!2025:\n    ├── Heading [h3] - 10. Getting to share my students' work on `tidyclust` with y'all.\n    │   └── Markdown [9 lines]\n    └── Heading [h3] - 9. [ggplot storytelling using tricks from McCall Pitcher](https://mccall-pitcher.quarto.pub/storytelling-with-ggplot2-bar-chart-sequence/) \n\n\nThe use case I’m excited about is creating class materials - I can use one source qmd to generate a student version and an instructor/solutions version.\nCan’t wait to use this to re-write the versions functions from templar (which I never truly released anyways, just used myself) in much cleaner and easier to use form!!!!\n\n\n3. “We R Together” Keynote by Yanina Bellini Saibene\nIn my opinion, a great Keynote is less about imparting specific information, and more about leaving your audience feeling excited or inspired.\nYani’s talk did that beautifully. I loved how she supplied a structured, well-defined, research-supported way to think about the communities we join and the process of belonging.\n\nI particularly loved how she asked us to reflect on moments where we hit milestones in our own communities - when did we feel welcome in a community, when did we feel ownership of the community, etc. I think the “ownership” idea is particularly relevant in open-source, where so much of what is build relies on folks giving their blood, sweat, and tears to unpaid and often thankless work.\n\n\n\n2. ellmer, chores, etc. by Simon Couch and the Posit Squad\nDespite the fact that Hadley Wickham’s Keynote was (as always) poignant and hilarious, I hadn’t yet been sold on using ellmer et al in my own workflow.\nHowever, Simon Couch’s talk on his chores package has me sold. It’s a clean, IDE-plugin interface for getting LLMs to do tiny tedious tasks, such as writing Roxygen documentation. I think of it as “usethis on steroids” - it automates some of the obnoxious formatting needed for certain programming jobs, and then takes it a step further by fleshing out a little of the content as well.\nOne open question though: What shall we call the Posit team’s collection of LLM helpers? #couchverse has been proposed…\n\n\n1. flourish by Visruth Srimath Kandali and me\nTotal humblebrag here, but I have to be honest about what I was most excited about.\nThis year, I’ve been working with Visruth, an undergraduate (!) who is already a better programmer than all of us faculty. Together we’ve build flourish, a Quarto extension that’s a successor to the flair R package, for highlighting source code in a rendered doc.\nVisruth did a great job in his talk at showing the many many possibilities offered by flourish… including some truly ridiculous css formatting!\n#| flourish:\n#| - target:\n#|      - \"wackyyy\"\n#|      - style:\n#|            - \"background-color: #bd8c51;\"\n#|            - \"text-decoration: line-through;\"\n#|            - \"-webkit-text-stroke: 1px black;\"\n#|            - \"filter: blur(1px);\"\n#|            - \"font-family: 'Brush Script MT', cursive;\"\n\nwackyyy &lt;- 1:10\nmean(wackyyy)\n\n[1] 5.5\n\n\nI can’t wait to keep expanding this extension, and there was something very special about watching a great student present our work instead of doing it myself."
  },
  {
    "objectID": "posts/2024-07-13-welcome/index.html",
    "href": "posts/2024-07-13-welcome/index.html",
    "title": "paRticles",
    "section": "",
    "text": "One of my resolutions right now is to be better about putting things out into the internet world, without feeling like they need to be polished or perfect or full of lots of content.\nI already have a mostly unused blog as well as some long-form posts on other blogs and I’ll probably stick to those places when I manage to write things that are, you know, thorough and carefully edited over weeks. But probably my favorite thing I’ve ever put out into the internet world is this unhinged stream-of-consciousness about trying to learn NSE, and I want to get more comfortable with that kind of in-the-moment micro post.\nSo: here is where I’ll try to put quick ideas, discoveries, and little things that I figure out or think are cool. Welcome to paRticles!"
  },
  {
    "objectID": "posts/2024-08-14-positconf_quarto/index.html",
    "href": "posts/2024-08-14-positconf_quarto/index.html",
    "title": "Notes from Posit::conf session on Quarto",
    "section": "",
    "text": "I took so many notes for myself in this session I figured why not throw them on here!\n\n\nDavid Keyes: Making pretty reports\n\nCreate a layout with Typst.\n💡report.qmd -&gt; typst-show.typ passes variables from Quarto params to typst -&gt; typst-template.typ sets up report properties like fonts, colors, page margins, backgrounds. “grid” to control placements.\n\n\nUse functions to make plots consistent.\n💡ggplot layering makes this nice\nif (&lt;function input&gt;) p &lt;- p + &lt;layer option&gt;\nelse p &lt;- p + &lt;different layer option&gt;\n💡custom themes! But geom_text() etc are not impacted by themes, use update_geom_defaults()\n🔗 rfor.us/posit2024slides\n\n\n\n\nMine Çetinkaya-Rundel: Books!\n\nMultiple Outputs\n💡Put things in a fenced div (:::) -&gt; write scss to style it for html and put it as theme -&gt; write style.tex file to style it for latex/pdf and include it in header.\n💡“litter” your qmd file with LaTeX tags like \\newpage tags, they will be ignored in html render. Use \\index tags and separate tex, then \\printindex at the end.\n\n\nAccessibility/Reproducibility\n💡 Use fig-alt for alt text. parsermd to look for instances of ggplot that don’t have fig alt.\n💡 Set global options in _common.R.\n💡 Use Quarto’s announcement option in _quarto.yml to track status of WIP project/chapters.\n💡 Avoid freeze; re-run all code in GH Actions. 😱 Quarto Actions Repo!!!\n\n\nMultilingual\n💡 Use “embed” shortcode to insert notebooks with different engines. 🤯\n🔗 bit.ly/books-conf24\n\n\n\n\nMeghan Hall: templates\n💡 Start by tinkering with custom scss (reference as theme in _quarto.yml) and make use of custom divs!\nIn qmd:\n::: some-div-name\ntext\n:::\nIn scss:\n.some-div-name {\n&lt;css junk&gt;\n}\n💡 “inspect” in browser and Mozilla web docs for css/html elements.\n💡 Put all this in an internal package, make it copy from inst to working directory and open qmd in editor.\n🤔 Internal package vs. template in RStudio vs quarto use template???\n\n\n\nAndrew Bray: scrollytelling\n💡 “closeread” Quarto extension\nformat: closeread-html\n\n::: {.cr-section}\nblah blah left sidebar blah\n\n@cr-thing\n\n:::{.cr-thing}\nstuff that should appear on the right at trigger point\n:::\n\n:::\n💡 Can also add “focus effects” to “stickies” at trigger points. e.g. zooming in to pieces of an image, pan-to and scale-by\n[@cr-thing]{pan-to=”70%,-10%” scale-by=”2”}\n💡 Use the progression tracker (crProgressBlock OJS variable) in code to change the images as the user scrolls.\n🔗 Example"
  },
  {
    "objectID": "posts/2024-11-22-aes/index.html",
    "href": "posts/2024-11-22-aes/index.html",
    "title": "Move your aes",
    "section": "",
    "text": "library(palmerpenguins)\nlibrary(tidyverse)\nI’m taking a second here to expand on this interesting little discussion on BlueSky, about where to put the aes in a ggplot.\nBasically, the question boils down to:\nOption A: aes inside ggplot()\npenguins |&gt;\n  ggplot(aes(x = body_mass_g, \n             y = bill_length_mm)) +\n  geom_point()\nOption B: aes inside geom_*()\npenguins |&gt;\n  ggplot() +\n  geom_point(aes(x = body_mass_g, \n             y = bill_length_mm))\nOption C: aes outside ggplot()\npenguins |&gt;\n  ggplot() +\n  aes(x = body_mass_g, \n      y = bill_length_mm) +\n  geom_point()\nI think for most of of, Option A “looks right”. Probably this is because it’s more frequently taught that way, such as in R for Data Science.\nActually, in R4DS they also name the argument, i.e., mapping = aes(...), but that’s a whole other can of worms that I’m going to ignore for now.\nAnyways, I’m going to argue for Option C for two reasons:"
  },
  {
    "objectID": "posts/2024-11-22-aes/index.html#counterarguments",
    "href": "posts/2024-11-22-aes/index.html#counterarguments",
    "title": "Move your aes",
    "section": "Counterarguments",
    "text": "Counterarguments\nJust to try to anticipate a few of these…\n\nIt’s still a global mapping when it’s inside the ggplot() function.\n\nSure. But I see this particular inheritance confusion SO frequently among new users that it’s clear something isn’t being understood. My guess is that this makes the global mapping look like it’s on the same “level” as the local ones, since they are both inside one level of function.\n\nThe data and the mapping are the two required elements, that’s why they both belong in the ggplot() function.\n\nRequired in what sense? For the code to run without error you only need the dataset:\n\nggplot(penguins)\n\n\n\n\n\n\n\n\nFor an interesting plot of any kind, you’d need the mapping and the geometry - nobody is proposing putting geom_*() inside of ggplot() so I don’t see why aes() should be any different.\n\nI’m used to it that way so it looks right.\n\nSame, friend, same.\nI’ll probably be stuck in Option A for a while due to muscle memory - but I’m going to try out Option C in my teaching materials going forward and see how that goes!"
  },
  {
    "objectID": "posts/2024-08-05-positron/index.html",
    "href": "posts/2024-08-05-positron/index.html",
    "title": "First thoughts on Positron",
    "section": "",
    "text": "This is a very disorganized list of notes I made trying out Positron for the first time. Hopefully some slightly more coherent updates will come along eventually as I use it more and/or it updates.\nWorking directories: I thought I would hate the “open a folder” workflow (as opposed to opening a .RProj) but actually I’m vibing with it. It’s especially nice for one-off tasks, e.g. when I use R to calculate an exam curve. I never quite liked choosing between a “floating” qmd or a one-time-use R Project.\n\n\n\nFolders behave like projects.\n\n\nThat said, my work didn’t “feel” like a project. I like having all the RStudio windows open in my dock so I can project hop. Relatedly - I crash R all the time, and I like that it only crashes the instance I’m in; does using Positron mean that if I crash things, all my projects close?\n\n\n\nRRRRRRRRRR\n\n\nQuarto: The “new notebook” shortcut on the landing page makes a Jupyter doc not a Quarto doc. Booo. Plus there’s NO RENDER BUTTON for Quarto docs. Ew. Worst,there’s no inline preview option in Quarto yet, which is a major dealbreaker for now. I’m told these things will be added; but for now Positron is not super optimized for Quarto.\n\n\n\nThis opens a new ipynb.\n\n\nChunks: The chunk shortcut is different. That’s fine, I think it’s PC/Mac consistent now. I do like the drop-down language options when you add a chunk… except that after the first chunk I don’t want to have to “confirm” my language choice every time. The “run this chunk” play button is IMHO better, although breaking my muscle memory has been hard.\n\n\n\nSnazzy, but now I have to hit “enter” every time I make a new chunk.\n\n\nPanes: I don’t like having the variables pane at the bottom, and then it disappears if you switch tabs at top. I like being able to see my environment at all times regardless of what help tab I’m in.\n \nFiles: I like having notification badges for unsaved files. It’s also just a lot easier to navigate between files in general.\n\n\n\nI love having the Explorer on the left!\n\n\nHelp: The dynamic suggestions are cool… but floating helpers are in my way. I like the info but it’s in my way enough that I would turn this off - I’d rather have some kind of permanent help text location in a corner that I can glance at when I need it.\n\n\n\nIt’s just kind of an excessive amount of pop-up and it’s blocking my pipeline."
  },
  {
    "objectID": "posts/2024-10-31-look_at_objects/index.html",
    "href": "posts/2024-10-31-look_at_objects/index.html",
    "title": "Look at your objects",
    "section": "",
    "text": "Random thought today: There are a lot of ways to “check in” on your intermediate objects in R.\nIt’s definitely good practice and something I have trouble pushing my students to do. Maybe I need to be more deliberate about how to do it.\nSo, there’s the classic way of just printing it out. This is fine. I tend to peek at my objects this way, except I do the peeking in the console… I can NOT get my students to adopt a workflow that pops between notebook and console though. Maybe it’s not the best.\nbob &lt;- 1:10\nbob\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "posts/2024-10-31-look_at_objects/index.html#looking-inside-pipelines",
    "href": "posts/2024-10-31-look_at_objects/index.html#looking-inside-pipelines",
    "title": "Look at your objects",
    "section": "Looking Inside Pipelines",
    "text": "Looking Inside Pipelines\nSpeaking of pipelines, I’m on the fence about the best way to “check in” on progress of a long pipeline. I tend to just highlight part of the pipeline and Cmd+Enter to run that section. But that’s kinda unreproducible and also gets annoying if I’m doing it many times.\nStudents tend to delete or comment out segments of pipelines and I do NOT like this, it’s so unwieldy.\n\nUsing the “passthrough” pipe\nmagrittr has a cute pipe %T&gt;% that means “do this next step but don’t pass its results”, which we can use in conjunction with print() to check stuff.\nIt’s almost perfect but the necessity of print() and the subtlety of the %T&gt;% pipe (it’s easy to miss) annoy me a bit.\n\nlibrary(magrittr)\n\npenguins %&gt;%\n  filter(species == \"Adelie\") %&gt;%\n  print() %&gt;%  \n  summarize(n_rows = n())\n\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 142 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n# A tibble: 1 × 1\n  n_rows\n   &lt;int&gt;\n1    152\n\n\n(Honestly, I wish we in the tidyverse sphere used the other magrittr pipes more. Maybe another mini-post one day…)\n\n\nSummary functions that return x\nFinally, you might just use glimpse() in a pipeline, since it invisibly returns the data frame as well as printing a summary, so it can flow through the pipeline:\n\npenguins %&gt;%\n  filter(species == \"Adelie\") %&gt;%\n  glimpse() %&gt;%  \n  summarize(n_rows = n())\n\nRows: 152\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n# A tibble: 1 × 1\n  n_rows\n   &lt;int&gt;\n1    152\n\n\nMy problem here is simply that I don’t love glimpse()… if I’m verifying a pipeline step, I’d rather just see the raw data.\nGoogling around lead me to textreadr::peek(), which seems to be exactly that:\n\n# remotes::install_github(\"trinker/textreadr\")\nlibrary(textreadr)\n\npenguins %&gt;%\n  filter(species == \"Adelie\") %&gt;%\n  peek() %&gt;%  \n  summarize(n_rows = n())\n\nTable: [152 x 8]\n\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year\n1  Adelie  Torgersen 39.1           18.7          181               3750        male   2007\n2  Adelie  Torgersen 39.5           17.4          186               3800        female 2007\n3  Adelie  Torgersen 40.3           18            195               3250        female 2007\n4  Adelie  Torgersen &lt;NA&gt;           &lt;NA&gt;          &lt;NA&gt;              &lt;NA&gt;        &lt;NA&gt;   2007\n5  Adelie  Torgersen 36.7           19.3          193               3450        female 2007\n6  Adelie  Torgersen 39.3           20.6          190               3650        male   2007\n7  Adelie  Torgersen 38.9           17.8          181               3625        female 2007\n8  Adelie  Torgersen 39.2           19.6          195               4675        male   2007\n9  Adelie  Torgersen 34.1           18.1          193               3475        &lt;NA&gt;   2007\n10 Adelie  Torgersen 42             20.2          190               4250        &lt;NA&gt;   2007\n.. ...     ...       ...            ...           ...               ...         ...    ...  \n\n\n# A tibble: 1 × 1\n  n_rows\n   &lt;int&gt;\n1    152\n\n\nIt’s not on CRAN anymore (sadface). Also, tibbles get downgraded to data.frames. But still, I like this a lot."
  },
  {
    "objectID": "posts/2024-10-31-look_at_objects/index.html#conclusion",
    "href": "posts/2024-10-31-look_at_objects/index.html#conclusion",
    "title": "Look at your objects",
    "section": "Conclusion",
    "text": "Conclusion\nSo, no perfect solution for pipelines that I know of. And all these options will also print their output in a rendered qmd/Rmd - so they have the same issue as print debugging in that you have to remember to go back and remove code when you are finished developing.\nI think my personal wishlist would be, in no particular order:\n\nA dplyr::peek() function.\nA “print and pass” pipe that could be used in a pipeline without needing a function.\nSome kind of interactive tool in Quarto that would let you flag lines to be previewed upon chunk run, without them being printed in a rendered doc.\n\nThoughts? Ideas?"
  },
  {
    "objectID": "posts/2025-07-23-speed_test/index.html",
    "href": "posts/2025-07-23-speed_test/index.html",
    "title": "Speed Testing: Three Levels",
    "section": "",
    "text": "While it’s fresh in my mind (mostly thanks to Tyson Barrett’s awesome material from our USCOTS workshop), I want to jot down the different ways to speed test your R Code.\nFirst, let’s load up the packages I’ll use:\n#pak::pak(c(\"nycflights13\", \"microbenchmark\", \"profvis\", \"bench\", \"tictoc\", \"atime\", \"dtplyr\"))\nlibrary(tidyverse)\nlibrary(nycflights13)\nlibrary(microbenchmark)\nlibrary(bench)\nlibrary(tictoc)\nlibrary(atime)\nlibrary(profvis)\nNow let’s make a little pipeline with nontrivial compute time (on my laptop, at least) that we can use to test things. I’m totally cheating here by just duplicating the dataset 10 times to make it bigger. And also ChatGPT wrote my pipeline. Anyways…\nflights_big &lt;- bind_rows(replicate(10, flights, simplify = FALSE))\n\n\nflights_big |&gt;\n  filter(!is.na(air_time), !is.na(tailnum), !is.na(dep_delay)) |&gt;\n  group_by(tailnum) |&gt;\n  summarise(\n    mean_air_time = mean(air_time),\n    sd_air_time = sd(air_time),\n    n = n(),\n    delay_score = sum((dep_delay)^2) / n()\n  ) |&gt;\n  left_join(planes, by = \"tailnum\") |&gt;\n  mutate(\n    model_length = str_length(model),\n    manufacturer_upper = str_to_upper(manufacturer)\n  ) |&gt;\n  filter(!is.na(model_length), n &gt; 50) |&gt;\n  arrange(desc(delay_score))\n#&gt; # A tibble: 2,906 × 15\n#&gt;    tailnum mean_air_time sd_air_time     n delay_score  year type   manufacturer\n#&gt;    &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;       \n#&gt;  1 N384HA          626.        24.8    330      51368.  2011 Fixed… AIRBUS      \n#&gt;  2 N276AT          114.         8.41    60      37738.  2005 Fixed… BOEING      \n#&gt;  3 N6716C          202.        86.4    250      32628.  2001 Fixed… BOEING      \n#&gt;  4 N550NW          108.         3.56    70      24047.  2001 Fixed… BOEING      \n#&gt;  5 N203FR          233.        16.9    410      21445.  2002 Fixed… AIRBUS INDU…\n#&gt;  6 N184DN          239.       107.     160      18789.  1993 Fixed… BOEING      \n#&gt;  7 N521VA          334.        15.4    270      18686.  2006 Fixed… AIRBUS      \n#&gt;  8 N927DA          137.        21.3    820      17802.  1988 Fixed… MCDONNELL D…\n#&gt;  9 N635AA          185.        58.1    290      15948.  1990 Fixed… BOEING      \n#&gt; 10 N923FJ           90.2        7.47   120      15756.  2004 Fixed… BOMBARDIER …\n#&gt; # ℹ 2,896 more rows\n#&gt; # ℹ 7 more variables: model &lt;chr&gt;, engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;,\n#&gt; #   engine &lt;chr&gt;, model_length &lt;int&gt;, manufacturer_upper &lt;chr&gt;\nSo that I don’t have to copy-paste that long pipeline several times, I’m going to wrap it in a quick silly function. You don’t need to do this in your workflow, it’s just saving blog post space.\ndo_the_thing &lt;- function(dat) {\n  \n  temp &lt;-  flights_big |&gt;\n  filter(!is.na(air_time), !is.na(tailnum), !is.na(dep_delay)) |&gt;\n  group_by(tailnum) |&gt;\n  summarise(\n    mean_air_time = mean(air_time),\n    sd_air_time = sd(air_time),\n    n = n(),\n    delay_score = sum((dep_delay)^2) / n()\n  ) |&gt;\n  left_join(planes, by = \"tailnum\") |&gt;\n  mutate(\n    model_length = str_length(model),\n    manufacturer_upper = str_to_upper(manufacturer)\n  ) |&gt;\n  filter(!is.na(model_length), n &gt; 50) |&gt;\n  arrange(desc(delay_score))\n  \n   return(\"done\")\n}"
  },
  {
    "objectID": "posts/2025-07-23-speed_test/index.html#level-1-quick-time-checks",
    "href": "posts/2025-07-23-speed_test/index.html#level-1-quick-time-checks",
    "title": "Speed Testing: Three Levels",
    "section": "Level 1: Quick time checks",
    "text": "Level 1: Quick time checks\nMost often, I just need a quick-and-dirty way to see approximately how long my code snippet is taking. Usually this is so I can estimate how long it will take to repeat many times; e.g. if I’m running some simulations in an experiment.\ntl;dr - Use proc.time() or the tictoc package.\n\nMaybe don’t use this: Sys.time()\nA function that newer R users tend to know or find is Sys.time() to record the current time. So to speed test code, they will (reasonably) just check the time then manually calculate how much has elapsed:\n\nstart_time &lt;- Sys.time()\n\ndo_the_thing(flights_big)\n#&gt; [1] \"done\"\n\nnow &lt;- Sys.time() - start_time\n\nprint(now)\n#&gt; Time difference of 0.2220261 secs\n\nOne critique of this method is that it’s just measuring how much time elapsed in real life, not necessarily how much was spent on the code I’m testing. That is, if you have some other giant computation running on your computer at the same time, this might appear slower. More on that in a sec. For many users, wanting more of a quick look at a slowdown than a precise compute time, that’s not a big deal.\nThe real main drawback of this approach, aside from stylistic preference, is that the object you get from subtracting times is a difftime. I don’t like these, because they essentially return a number, with an attribute tacked on letting you know if it’s seconds or minutes.\n\nclass(now)\n#&gt; [1] \"difftime\"\n\nstr(now)\n#&gt;  'difftime' num 0.222026109695435\n#&gt;  - attr(*, \"units\")= chr \"secs\"\n\nHere’s why that is bad. Suppose I have one task that takes a fraction of a second, and one task that takes a few minutes.\nI time these out and I convert them to doubles so I can do math on them without the pesky difftime tagalong info.\n\nstart_time &lt;- Sys.time()\n\nfor (i in 1:300) {\n  do_the_thing(flights_big)\n}\n\nnow_2 &lt;- Sys.time() - start_time\n\nnow_2\n#&gt; Time difference of 1.093376 mins\n\n\nas.double(now)\n#&gt; [1] 0.2220261\nas.double(now_2)\n#&gt; [1] 1.093376\n\nHmmm…. doing the task 300 times only took a little longer than doing it once? Very suspicious…\nOf course, this problem is easy to avoid - and realistically, you’d more likely be just printing your times out than saving them somewhere - but it’s also a fairly easy pit to fall into while making reasonable choices.\n\n\nBase R has your back: proc.time() and system.time()\nYou can get around both criticisms of Sys.time() simply by using proc.time() instead:\n\nstart_time &lt;- proc.time()\n\ndo_the_thing(flights_big)\n#&gt; [1] \"done\"\n\n\nnow &lt;- proc.time() - start_time\n\nprint(now)\n#&gt;    user  system elapsed \n#&gt;   0.171   0.048   0.219\n\nThe now object is technically a proc_time object, but really it’s essentially just a named vector.\n\nclass(now)\n#&gt; [1] \"proc_time\"\nstr(now)\n#&gt;  'proc_time' Named num [1:5] 0.171 0.048 0.219 0 0\n#&gt;  - attr(*, \"names\")= chr [1:5] \"user.self\" \"sys.self\" \"elapsed\" \"user.child\" ...\n\nThe interesting bit here for now is the elapsed part - the distinction between user computations and system computations is subtle, and if it matters to your development, you’re probably better off using a helper package like those as the end of this post.\n\nnow[3]\n#&gt; elapsed \n#&gt;   0.219\n\nSince this is just a vector of doubles, and it’s consistently returned in seconds, there’s no danger of mixing and matching different units in weird ways.\nFor an alternate syntax, instead of calling proc.time() twice, you can call your code inside of system.time(), which is nice is your snippet is short:\n\nsystem.time({\n  do_the_thing(flights_big)\n})\n#&gt;    user  system elapsed \n#&gt;   0.159   0.037   0.198\n\n\n\nBut wrappers are convenient: tictoc()\nMy favorite timing package is tictoc(), which is basically just a cute wrapper doing the same thing as proc.time():\n\ntic()\n\ndo_the_thing(flights_big)\n#&gt; [1] \"done\"\n\ntoc()\n#&gt; 0.216 sec elapsed\n\nLike the Sys.time() approach, this one is really meant for quick timing printouts, not for saving results of many experiments. If you do want to save the result, you’ll find a list rather than a difftime or proc_time, even though the text printout looks the same as the difftimes did:\n\ntic()\n\ndo_the_thing(flights_big)\n#&gt; [1] \"done\"\n\nnow &lt;- toc()\n#&gt; 0.224 sec elapsed\n\nclass(now)\n#&gt; [1] \"list\"\nstr(now)\n#&gt; List of 4\n#&gt;  $ tic         : Named num 3.18\n#&gt;   ..- attr(*, \"names\")= chr \"elapsed\"\n#&gt;  $ toc         : Named num 3.4\n#&gt;   ..- attr(*, \"names\")= chr \"elapsed\"\n#&gt;  $ msg         : logi(0) \n#&gt;  $ callback_msg: chr \"0.224 sec elapsed\"\n\nWhen you access the actual time measurement ($toc), it consistently returns a your time as a double in milliseconds.\n\nnow$toc\n#&gt; elapsed \n#&gt;   3.405\n\nstr(now$toc)\n#&gt;  Named num 3.4\n#&gt;  - attr(*, \"names\")= chr \"elapsed\"\n\nAnother feature of tictoc() - although not one I see used often - is the ability to automatically keep a log of several tic() and toc() results.\n\ntic.clearlog()\n\nfor (x in 1:10) {\n   tic(x)\n   do_the_thing(flights_big)\n   toc(log = TRUE, quiet = TRUE)\n}\n\nresults &lt;- tic.log()\nresults_raw &lt;- tic.log(format = FALSE)\ntic.clearlog()\n\nBy default, the log gives a list of the messages in text form:\n\nresults[1:2]\n#&gt; [[1]]\n#&gt; [1] \"1: 0.221 sec elapsed\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"2: 0.208 sec elapsed\"\n\nHowever, you can also get the raw toc() object results:\n\nresults_raw[1:2]\n#&gt; [[1]]\n#&gt; [[1]]$tic\n#&gt; elapsed \n#&gt;   3.414 \n#&gt; \n#&gt; [[1]]$toc\n#&gt; elapsed \n#&gt;   3.635 \n#&gt; \n#&gt; [[1]]$msg\n#&gt; [1] 1\n#&gt; \n#&gt; [[1]]$callback_msg\n#&gt; [1] \"1: 0.221 sec elapsed\"\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; [[2]]$tic\n#&gt; elapsed \n#&gt;   3.635 \n#&gt; \n#&gt; [[2]]$toc\n#&gt; elapsed \n#&gt;   3.843 \n#&gt; \n#&gt; [[2]]$msg\n#&gt; [1] 2\n#&gt; \n#&gt; [[2]]$callback_msg\n#&gt; [1] \"2: 0.208 sec elapsed\"\n\nIt’s messy to look at, but it plays nice with some tidyverse functions to yank out a vector of results in numeric form:\n\nresults_raw |&gt;\n  map_dbl(~pluck(.x, \"toc\"))\n#&gt;  [1] 3.635 3.843 4.060 4.310 4.766 5.135 5.404 5.671 5.928 6.205"
  },
  {
    "objectID": "posts/2025-07-23-speed_test/index.html#level-2-benchmarking",
    "href": "posts/2025-07-23-speed_test/index.html#level-2-benchmarking",
    "title": "Speed Testing: Three Levels",
    "section": "Level 2: Benchmarking",
    "text": "Level 2: Benchmarking\nNow, if you are finding slowdowns in your code, you are probably also trying solutions to speed them up - whether this is different rearrangements of your pipeline, or calling in help from other packages. You’ll want to see which approach sped your code up best, and by how much. This is called Benchmarking.\nFor example, let’s consider running the gnarly pipeline with the data.table package instead. Since I don’t feel like translating the whole pipeline to the data.table syntax, instead I’ll just lean on the dtplyr package to do it for me.\n\nlibrary(dtplyr)\n\ndo_the_thing_dt &lt;- function(dat) {\n  \n temp &lt;- dat |&gt;\n  lazy_dt() |&gt;\n  filter(!is.na(air_time), !is.na(tailnum), !is.na(dep_delay)) |&gt;\n  group_by(tailnum) |&gt;\n  summarise(\n    mean_air_time = mean(air_time),\n    sd_air_time = sd(air_time),\n    n = n(),\n    delay_score = sum((dep_delay)^2) / n()\n  ) |&gt;\n  left_join(planes, by = \"tailnum\") |&gt;\n  mutate(\n    model_length = str_length(model),\n    manufacturer_upper = str_to_upper(manufacturer)\n  ) |&gt;\n  filter(!is.na(model_length), n &gt; 50) |&gt;\n  arrange(desc(delay_score))\n \n return(\"done\")\n  \n}\n\nA quick timing shows that we sped our code up by almost 5 times!\n\ntic()\ndo_the_thing(flights_big)\n#&gt; [1] \"done\"\ntoc()\n#&gt; 0.217 sec elapsed\n\ntic()\ndo_the_thing_dt(flights_big)\n#&gt; [1] \"done\"\ntoc()\n#&gt; 0.067 sec elapsed\n\n\nRepeated tests with microbenchmark()\nAs you may have noticed throughout this post, running the exact same code twice doesn’t always take the exact same amount of time.\nThe microbenchmark package is helpful if you want to do a true experiment, and run the different approaches each many times before making a comparison. By default, it will run your code 100 times - be aware of how long this will take total before you start running code!\n\nmicrobenchmark(\n  dplyr_version = do_the_thing(flights_big),\n  dt_version = do_the_thing_dt(flights_big)\n)\n#&gt; Warning in microbenchmark(dplyr_version = do_the_thing(flights_big), dt_version\n#&gt; = do_the_thing_dt(flights_big)): less accurate nanosecond times to avoid\n#&gt; potential integer overflows\n#&gt; Unit: milliseconds\n#&gt;           expr       min        lq      mean    median        uq      max neval\n#&gt;  dplyr_version 210.06674 250.41693 257.65915 257.91749 268.71517 394.9760   100\n#&gt;     dt_version  29.93234  40.30591  60.23163  53.06985  79.15314 124.5242   100\n\n\n\nSize experiments with bench\nWe saw that using data.table bought us quite a bit of time in this case. But would it be worth it if we hadn’t made the giant 10x version of the dataset?\nThe bench package has essentially the same syntax as microbenchmark, except that:\n\nIt only runs each code snippet once\nIt (very annoyingly) requires the output to be identical for the two snippets. I got around this by just returning “done” in each function.\n\n\nbench::mark(\n  dplyr_version = do_the_thing(flights_big),\n  dt_version = do_the_thing_dt(flights_big)\n)\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n#&gt; # A tibble: 2 × 6\n#&gt;   expression         min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;    &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 dplyr_version  232.1ms    257ms      3.91     634MB     9.11\n#&gt; 2 dt_version      61.2ms    105ms     10.6      386MB    10.6\n\nSo why would we use it? First, because you also get memory results, which I’ve kind of swept under the rug in this post. Usually, memory and speed results agree with each other, and we’re more interested in speed. But if you are benchmarking because you are worried about maxing out your memory, this is nice to have.\nIn this case, the code took about 600 MB and 400 MB of memory… my current laptop has 16,000 MB available, and R on Mac can typically access all of that, so no worries there! But perhaps if I wanted to run this code thousands of times, I might need to be careful with how much I store and how much I run in parallel.\nAnyways. Without digressing too far into memory stuff - the reason to use bench over microbenchmark is to see how your speed improvements scale with size. We’ll use bench::press() to establish a set of values, then we’ll make a version of our flights_big dataset for each value, then we’ll benchmark our two versions of the code on those datasets.\n\nresults &lt;- bench::press(\n  duplications = c(2, 10, 20),\n  {\n    flights_big &lt;- bind_rows(replicate(duplications, flights, simplify = FALSE))\n    bench::mark(\n        dplyr_version = do_the_thing(flights_big),\n        dt_version = do_the_thing_dt(flights_big)\n    )\n  }\n)\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n\nresults\n#&gt; # A tibble: 6 × 7\n#&gt;   expression    duplications      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;           &lt;dbl&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 dplyr_version            2 244.33ms  275.5ms      3.63   634.3MB     9.08\n#&gt; 2 dt_version               2   8.02ms     10ms     62.9     77.4MB     9.82\n#&gt; 3 dplyr_version           10 209.62ms  271.1ms      3.69   634.3MB     7.38\n#&gt; 4 dt_version              10  34.36ms   64.5ms     13.2    385.7MB    11.3 \n#&gt; 5 dplyr_version           20 351.19ms  359.6ms      2.78   634.3MB     9.73\n#&gt; 6 dt_version              20 101.77ms  119.1ms      6.93   771.1MB     5.20\n\nSo, does the advantage of data.table scale linearly as the data gets bigger? Not really - both methods only get slightly slower as the data gets bigger, so dt goes from being something like 25 times faster on the small dataset to only 3 times faster on the large one.\n\n\nCode\nresults |&gt;\n  mutate(\n    version = rep(c(\"dplyr\", \"data.table\"), 3),\n    median_time = as.double(median)\n  ) |&gt;\n  ggplot() +\n  aes(x = duplications, y = median_time, color = version) +\n  geom_point() +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nI’m being a little unfair to data.table here. The vast, vast majority of the time spent in the do_the_thing_dt() function is in converting the data frame to a data.table object with lazy_dt().\nSo basically, if you’re going to convert your data to a data.table once and then run a ton of pipelines/repetitions, then data.table is super helpful in larger data. But if it’s a one-off process, the cost of creating the data.table object is more than the speed-up of the code.\nHow do I know the time breakdown of the do_the_thing_dt() function? Read on…"
  },
  {
    "objectID": "posts/2025-07-23-speed_test/index.html#level-3-profiling",
    "href": "posts/2025-07-23-speed_test/index.html#level-3-profiling",
    "title": "Speed Testing: Three Levels",
    "section": "Level 3: Profiling",
    "text": "Level 3: Profiling\nThe next notch up is to figure out where exactly, in our long pipeline, the slowdown is happening. This is called Profiling.\n\nQuick look with profviz\nprofviz is a tool that makes interactive plots to explore the computation time of each routine and subroutine in your code. This time I’m copying the whole pipeline, so that profviz will measure the different steps instead of just the wrapper function.\n\nprofvis({\n  flights_big |&gt;\n  filter(!is.na(air_time), !is.na(tailnum), !is.na(dep_delay)) |&gt;\n  group_by(tailnum) |&gt;\n  summarise(\n    mean_air_time = mean(air_time),\n    sd_air_time = sd(air_time),\n    n = n(),\n    delay_score = sum((dep_delay)^2) / n()\n  ) |&gt;\n  left_join(planes, by = \"tailnum\") |&gt;\n  mutate(\n    model_length = str_length(model),\n    manufacturer_upper = str_to_upper(manufacturer)\n  ) |&gt;\n  filter(!is.na(model_length), n &gt; 50) |&gt;\n  arrange(desc(delay_score))\n})\n\n\n\n\n\n \nWhat is this telling us? We can see from the bottom “stack” of the plot that filter is by far the slowest step in our pipeline.\n\n\nDetail (too much?) with Rprof\nThe Base R way to profile requires little fiddling.\nFirst, we profile the code and save it to a .out file:\n\nRprof(\"results.out\")\ndo_the_thing(flights_big)\n#&gt; [1] \"done\"\nRprof(NULL)\n\nThen, if we want to see the results, we have to read the results.out file using helper functions like:\n\nsummaryRprof(\"results.out\")$by.self\n#&gt;                            self.time self.pct total.time total.pct\n#&gt; \"vec_slice\"                     0.16    61.54       0.16     61.54\n#&gt; \"^\"                             0.02     7.69       0.02      7.69\n#&gt; \"&lt;Anonymous&gt;\"                   0.02     7.69       0.02      7.69\n#&gt; \"mean\"                          0.02     7.69       0.02      7.69\n#&gt; \"stopifnot\"                     0.02     7.69       0.02      7.69\n#&gt; \"vec_locate_sorted_groups\"      0.02     7.69       0.02      7.69\n\nThis is telling us that the slowest procedure was vec_slice; which we could track down and find is part of the filter step.\nThere are a number of helper packages and functions for visualizing or understanding the full profiling output (e.g. profr, proftools)"
  },
  {
    "objectID": "posts/2025-07-23-speed_test/index.html#addendum-random-thought-about-profiling-pipelines",
    "href": "posts/2025-07-23-speed_test/index.html#addendum-random-thought-about-profiling-pipelines",
    "title": "Speed Testing: Three Levels",
    "section": "Addendum: random thought about profiling pipelines",
    "text": "Addendum: random thought about profiling pipelines\nI wish there was a bit more pipeline-centric approach to profiling. Maybe a toodling project for another day.\nBut thought I had, if profiling feels daunting, is to manually break your pipeline up until you track down the bottleneck:\n\n\ndo_1 &lt;- function(dat) {\n  dat |&gt;\n  filter(!is.na(air_time), !is.na(tailnum), !is.na(dep_delay)) \n}\n\ndo_2 &lt;- function(temp)\n  temp |&gt;\n  group_by(tailnum) |&gt;\n  summarise(\n    mean_air_time = mean(air_time),\n    sd_air_time = sd(air_time),\n    n = n(),\n    delay_score = sum((dep_delay)^2) / n()\n  ) \n\n\ndo_3 &lt;- function(temp) {\n  temp |&gt;\n  left_join(planes, by = \"tailnum\") |&gt;\n  mutate(\n    model_length = str_length(model),\n    manufacturer_upper = str_to_upper(manufacturer)\n  ) \n}\n\ndo_4 &lt;- function(temp) {\n\n  temp |&gt;\n    filter(!is.na(model_length), n &gt; 50) |&gt;\n    arrange(desc(delay_score))\n}\n\nI’ll just be simple and split it up with tictoc(), but you could probably take a more systematic appraoch.\n\ntic()\ntemp &lt;- do_1(flights_big)\ntoc()\n#&gt; 0.236 sec elapsed\n\ntic()\ntemp &lt;- do_2(temp)\ntoc()\n#&gt; 0.113 sec elapsed\n\ntic()\ntemp &lt;- do_3(temp)\ntoc()\n#&gt; 0.001 sec elapsed\n\ntic()\ntemp &lt;- do_4(temp)\ntoc()\n#&gt; 0.001 sec elapsed\n\nI don’t really think the above was worth the effort… but consider this my official wish for a pipeline-step-measuring profiler! My vision would be that you put the pipeline through the profiler, and get back time/memory benchmarks for only the functions exposed in the pipeline. None of this vec_slice business; just a measurement on each successive step of the pipe."
  },
  {
    "objectID": "posts/2025-07-23-speed_test/index.html#conclusion",
    "href": "posts/2025-07-23-speed_test/index.html#conclusion",
    "title": "Speed Testing: Three Levels",
    "section": "Conclusion",
    "text": "Conclusion\nThis actually took longer than expected, mostly trying to understand the small differences in syntax, input structure, and output info of the various functions.\n\nTakeaway messages:\n\nMost of us just need a quick timer, and we can do that easily with proc.time() or system.time() or tictoc()\nIf you want to (a) average many runs of the speed test and/or (b) compare different solutions, use microbenchmark(). If you want to see how speed scales with size, use bench::mark()\nProfiling is really really helpful for pinpointing the problem. profviz gets you answers quickly; RProf() and friends get you all the info. But neither of these is as beginner/intermediate friendly as I would like.\n\n\n\nOther little notes\n\nIf you want to get extra fancy with benchmarking, especially for purposes of checking if code updates on GitHub are actually improvements, check out the atime package - great blog post about it here\nI’m aware there are other benchmarking packages, e.g. rbenchmark. I’m less familiar with those but would love to learn more, if you think there’s a reason a different one might be preferred!\nHere is a short post that gets deep in the weeds on benchmarking memory, including allocations in C."
  },
  {
    "objectID": "posts/2024-07-20-lda_qda/index.html",
    "href": "posts/2024-07-20-lda_qda/index.html",
    "title": "1-D Illustration of LDA vs QDA with {distributional}",
    "section": "",
    "text": "So, three things happened this week that inspired this tidbit:\n\nI learned about the {distributional} package by … at the UseR 2024 conference, which basically defines distributions as their own objects, to be plotted or used in calculations. Very cool!\nI was reminded in a conversation recently that {gganimate} is cool and something I’d like to use more in classes.\nA student asked me why QDA can lead to circular boundaries and LDA can’t, when the only difference is whether we let the covariance vary by group. I answered by very badly drawing a series of pictures in the 1-D case.\n\nSo my goal here is to re-create those pictures as a gif.\n\nlibrary(tidyverse)\nlibrary(distributional)\nlibrary(gganimate)\n\nAlright, first thing is to figure out plotting two normal distributions using {distributional}\n\nmy_dists &lt;- c(dist_normal(mu = 0, sigma = 1), \n              dist_normal(mu = 3, sigma = 1))\n\nmy_dists\n\n&lt;distribution[2]&gt;\n[1] N(0, 1) N(3, 1)\n\n\nDISTRIBUTIONS AS OBJECTS my people I am so pleased with this structure.\nOkay hang on apparently we need ggdist too to plot it… whoa I don’t think I realized how awesome and built-out ggdist is.\nAnyways now it’s super easy…\n\nlibrary(ggdist)\n\nggplot(my_dists) +\n  stat_dist_halfeye(aes(dist = dist))\n\nError in `fortify()`:\n! `data` must be a &lt;data.frame&gt;, or an object coercible by `fortify()`,\n  or a valid &lt;data.frame&gt;-like object coercible by `as.data.frame()`.\nCaused by error in `.prevalidate_data_frame_like_object()`:\n! `dim(data)` must return an &lt;integer&gt; of length 2.\n\n\nOOPS, famous last words. Of course my objects still need to be in a data frame… duh, Kelly.\n\nmy_df &lt;- tibble(\n  dist = c(dist_normal(mu = 0, sigma = 1), \n              dist_normal(mu = 3, sigma = 1))\n)\n\n\nggplot(my_df) +\n  stat_dist_halfeye(aes(dist = dist))\n\n\n\n\n\n\n\n\nOkay cool but it’s sideways and also I need colors. (Side note, it’s annoying that “distribution” and “distance” are both super common stat words and they have the same natural abbreviation. Rabble rabble.)\nA little noodling and cheating with coord flip (there was probably a better way to get these on the x-axis) and I have what I want for now:\n\nmy_df &lt;- tibble(\n  dist = c(dist_normal(mu = 0, sigma = 1), \n              dist_normal(mu = 3, sigma = 1)),\n  name = c(\"Class A\", \"Class B\")\n)\n\n\nggplot(my_df) +\n  stat_dist_halfeye(aes(dist = dist,\n                        fill = name),\n                    alpha = 0.5) +\n  coord_flip() +\n  theme_void() +\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\nNow we add the lines to show where the decision boundary for LDA would be (assuming equal prior probs here):\n\nggplot(my_df) +\n  stat_dist_halfeye(aes(dist = dist,\n                        fill = name),\n                    alpha = 0.5) +\n  coord_flip() +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  geom_hline(yintercept = 1.5) \n\n\n\n\n\n\n\n\nI kinda feel like I should be able to use geom_vline after coord_flip and have it not get flipped because it comes as a later layer. Rabble, rabble.\nAnyhoo. Now to animate.\nThis is a little weird to think about because gganimate wants to step through “state” or values in the data. I think I maybe want to set it up as two separate layers, one with the static blue curve and one with the moving red ones.\n\nmy_df &lt;- tibble(\n  means = (-8:8)/2,\n  dists = dist_normal(mu = means, sigma = 1),\n)\n\nggplot(my_df) +\n  stat_dist_halfeye(aes(dist = dists),\n                    alpha = 0.5,\n                    fill = \"#F8766D\") +\n  stat_dist_halfeye(aes(dist = dist_normal(mu = 0, sigma = 1)),\n                    alpha = 0.5,\n                    fill = \"#00BFC4\") +\n  coord_flip() +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  geom_hline(aes(yintercept = (means)/2))\n\nWarning in layer_slabinterval(data = data, mapping = mapping, stat = StatSlabinterval, : All aesthetics have length 1, but the data has 17 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nNow I gotta remember how you gganimate a plot. I think you add a layer with some kind of animate_ function?\n\nOk cool got it:\n\nanim &lt;-\n  ggplot(my_df) +\n  stat_dist_halfeye(aes(dist = dists),\n                    alpha = 0.5,\n                    fill = \"#F8766D\") +\n  stat_dist_halfeye(aes(dist = dist_normal(mu = 0, sigma = 1)),\n                    alpha = 0.5,\n                    fill = \"#00BFC4\") +\n  coord_flip() +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  geom_hline(aes(yintercept = (means)/2)) +\n  transition_manual(means)\n\nanimate(anim)\n\n\n\n\n\n\n\n\nEt voila! What we’re seeing here is that if the curves have the same variance, they can only ever cross at one point. (Well, unless they have the exact same mean and have infinite overlap, but if that’s the case then LDA is pointless anyways.)\nSo now what if we let them have different variances? We’ll try one for starters:\n\nmy_df &lt;- tibble(\n  means = -1,\n  dists = dist_normal(mu = means, sigma = 3),\n)\n\nggplot(my_df) +\n  stat_dist_halfeye(aes(dist = dists),\n                    alpha = 0.5,\n                    fill = \"#F8766D\") +\n  stat_dist_halfeye(data = NULL, aes(dist = dist_normal(mu = 0, sigma = 1)),\n                    alpha = 0.5,\n                    fill = \"#00BFC4\") +\n  scale_thickness_shared() +\n  coord_flip() +\n  theme_void() +\n  theme(legend.position=\"none\") \n\n\n\n\n\n\n\n\nOkay scale_thickness_shared() ended up being really important here to keep the curves from being the same height (and thus different areas, ew).\nFinding the intersection point is more of a pain than I thought about at first, because Normal equations are not really closed-form solvable. But distributional makes this not TOO bad at least.\n\nmy_df &lt;- tibble(\n  means = (-8:8)/2,\n  dists = dist_normal(mu = means, sigma = 3),\n)\n\nstd_normal &lt;- dist_normal(mu = 0, sigma = 1)\n\ngrid &lt;- seq(-10, 10, 0.1)\nstd_dens &lt;- density(std_normal, grid)[[1]]\n\ncrosslines &lt;-\n  my_df$dists |&gt;\n  density(grid) |&gt;\n  map(\\(dens) order(abs(dens/std_dens - 1))[1:2]) |&gt;\n  map(\\(idx) sort(grid[idx])) |&gt;\n  reduce(rbind)\n\nmy_df &lt;- \n  my_df |&gt;\n  mutate(\n    lower = crosslines[,1],\n    upper = crosslines[,2]\n  )\n\nPhew. That took some doing actually.\nBUT! The part you should be excited about here is my_df$dists |&gt; density(grid). Like…. that vectorization is elegant af.\nAnyways. Cool. Here’s a gif.\n\nanim &lt;-\n  ggplot(my_df) +\n  stat_dist_halfeye(aes(dist = dists),\n                    alpha = 0.5,\n                    fill = \"#F8766D\") +\n  stat_dist_halfeye(data = NULL, aes(dist = dist_normal(mu = 0, sigma = 1)),\n                    alpha = 0.5,\n                    fill = \"#00BFC4\") +\n  scale_thickness_shared() +\n  coord_flip() +\n  theme_void() +\n  theme(legend.position=\"none\") +\n  geom_hline(aes(yintercept = lower)) +\n  geom_hline(aes(yintercept = upper)) +\n  transition_manual(means)\n\nanimate(anim)\n\n\n\n\n\n\n\n\nBoom! Two points of intersection, which lets you segment the space such that blue class is “inside” and red class is “outside”.\nIf you can mental-image this up to two dimensions, maybe you can see how equal-variance bell curves intersect at a straight line, and nonequal-variance onces can have an “inner circle” and “outer circle”, hence LDA vs QDA.\nThe end!\nIf I were using this in class, I’d probably add:\n\nSome sample observations, to show that in LDA/QDA these curves are the estimates we get based on observed data, not something we magically know ahead of time.\nSome background coloring or arrows or something to clarify that the boundaries are prediction boundaries; we predict red or blue class based on where a new observation lands.\nSome aesthetic dressing up: Nice outlines on the curves, a star or something at the density intersection, better colors than red-blue, etc."
  },
  {
    "objectID": "posts/2025-04-24-nse/index.html",
    "href": "posts/2025-04-24-nse/index.html",
    "title": "Punctuation and Other Problems",
    "section": "",
    "text": "I’m doing some mentoring for Posit Academy’s “Programming in R” course, and the learners in my group have been asking very clever and deep questions about how to use Non-Standard Eval in R functions.\nSpecifically, the thing that keeps cropping up that I haven’t been able to answer in a satisfying way is iterating through unquoted input.\nThis blog post from Albert Rapp is excellent pre-reading."
  },
  {
    "objectID": "posts/2025-04-24-nse/index.html#tldr",
    "href": "posts/2025-04-24-nse/index.html#tldr",
    "title": "Punctuation and Other Problems",
    "section": "tl;dr",
    "text": "tl;dr\nBy request, I’m putting the final conclusions up front here for easy reference.\n\nTo map() over unquoted names:\nThe trick here is you need quos() to keep map() from triggering the unquoted code, and then you need tunneling ({x}) in the anonyous function as you would in any function:\n\nmap(quos(c(vs, am, gear, carb)),\n    \\(x) some_function(mtcars, {{x}}))\n\nTo pass the dots (...) into across():\nFirst you need enquos(...) to defuse the dots.\nThe sneaky bit in this one is that across() wants a vector of unquoted column names to use, and enquos() returns a list.\nSo, we splice the list into separate arguments with !!! and re-concatenate them with c().\nNOPE turns out there’s an easy pass here:\n\ndo_stuff &lt;- fucntion(data, ...) {\n\n    data |&gt;\n        summarize(across(c(...), some_function))\n\n}\n\nRead on to see an example, with the many things I tried that didn’t work, why they didn’t work, and how I fixed it."
  },
  {
    "objectID": "posts/2025-04-24-nse/index.html#set-the-scene",
    "href": "posts/2025-04-24-nse/index.html#set-the-scene",
    "title": "Punctuation and Other Problems",
    "section": "Set the scene",
    "text": "Set the scene\nFor the sake of example, let’s suppose the task I want to do is count how many ones are in a particular column.\nI’ve written a nice function, using tunneling ({{}}) to run on unquoted variable names.\n\n\ncount_ones &lt;- function(data, var) {\n  \n  data |&gt;\n    summarize(\n      n_ones = sum({{var}} == 1)\n    ) |&gt;\n    pull(n_ones)\n  \n}\n\ncount_ones(mtcars, vs)\n#&gt; [1] 14\n\nFabulous. We could clean this output up a bit, but we won’t, because lazy.\nSo, the question is, what if I want to do this to multiple columns at once?"
  },
  {
    "objectID": "posts/2025-04-24-nse/index.html#option-1-mapping",
    "href": "posts/2025-04-24-nse/index.html#option-1-mapping",
    "title": "Punctuation and Other Problems",
    "section": "Option 1: mapping",
    "text": "Option 1: mapping\nThe challenge here lies in the fact that if we put unquoted variable names into the map() function, the code “triggers” before it “gets to” the count_ones() function.\n\nmap(c(vs, am, gear, carb), \n    \\(x) count_ones(mtcars, x))\n#&gt; Error: object 'vs' not found\n\nOne solution is to fall back onto strings for the map() input and then re-unquote-ify them for use in count_ones(), which is highly unsatisfying.\n\nmap(c(\"vs\", \"am\", \"gear\", \"carb\"), \n    \\(x) count_ones(mtcars, !!sym(x)))\n#&gt; [[1]]\n#&gt; [1] 14\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 13\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n\nIt’s not terrible but the !!sym(x) is far from intuitive. I always read !! as “access the information stored in” and sym as “turn this from a string to a name”. So, it kind of makes sense - we hand a string to count_ones() but first we say “Don’t use this string, instead access the information in the name of the string.”\nI’m still convinced there’s a better way, though. Or at least, a different way.\nWhat I want to do is find a way to “freeze” the unquoted variable names so they can be passed into count_ones().\nMy first thought was to use quos(). Here’s now I understand these functions:\n\nquo() = freeze this one unquoted thing\nquos() = freeze this vector of unquoted things\nenquo() = freeze this unquoted function argument\nenquos() = frees this vector of unquoted function arguments\n\n\nmap(quos(c(vs, am, gear, carb)),\n    \\(x) count_ones(mtcars, x))\n#&gt; Error in `map()`:\n#&gt; ℹ In index: 1.\n#&gt; Caused by error in `summarize()`:\n#&gt; ℹ In argument: `n_ones = sum(x == 1)`.\n#&gt; Caused by error:\n#&gt; ! Base operators are not defined for quosures. Do you need to unquote\n#&gt;   the quosure?\n#&gt; \n#&gt; # Bad: myquosure == rhs\n#&gt; \n#&gt; # Good: !!myquosure == rhs\n\nWait, this is great! The error is being triggered in sum() inside of count_ones(), not inside of map(). So we did freeze it.\nThe error message suggests that I need to use !! inside of count_ones() to “unfreeze”. I’m skeptical, because I don’t want to unfreeze x; I want to access the name vs. Also my goal is not to modify that function.\nInstead I think this might just be a missed tunneling, so that the frozen column names get passed through my anonymous function.\n\nmap(quos(c(vs, am, gear, carb)),\n    \\(x) count_ones(mtcars, {{x}}))\n#&gt; [[1]]\n#&gt; [1] 34\n\nDang I really thought that would work, but it appears that by using quos(), I’ve accidentally frozen the whole vector together and counted everything in all columns. Which, honestly, is kind of cool - but not what I meant to do.\nI really don’t want to have to quo() each individual column name.\nLet me take a look a this output:\n\nquos(c(vs, am, gear, carb))\n#&gt; &lt;list_of&lt;quosure&gt;&gt;\n#&gt; \n#&gt; [[1]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^c(vs, am, gear, carb)\n#&gt; env:  global\n\nOkay so it froze the whole expression. Maybe we just don’t want the c(), because quos() is already concatenating?\n\nquos(vs, am, gear, carb)\n#&gt; &lt;list_of&lt;quosure&gt;&gt;\n#&gt; \n#&gt; [[1]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^vs\n#&gt; env:  global\n#&gt; \n#&gt; [[2]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^am\n#&gt; env:  global\n#&gt; \n#&gt; [[3]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^gear\n#&gt; env:  global\n#&gt; \n#&gt; [[4]]\n#&gt; &lt;quosure&gt;\n#&gt; expr: ^carb\n#&gt; env:  global\n\nThis is promising! A list of quosures is what we want!\n\nmap(quos(vs, am, gear, carb),\n    \\(x) count_ones(mtcars, {{x}}))\n#&gt; [[1]]\n#&gt; [1] 14\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 13\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n\n\n\n\nI am pleased with this solution."
  },
  {
    "objectID": "posts/2025-04-24-nse/index.html#option-2-pass-the-dots",
    "href": "posts/2025-04-24-nse/index.html#option-2-pass-the-dots",
    "title": "Punctuation and Other Problems",
    "section": "Option 2: Pass the dots",
    "text": "Option 2: Pass the dots\nThe other clever approach one of my learners took was to rewrite the original function to accept the variable names in the dots (...).\nThis works great if you are just sending the variable names along to the next internal function:\n\nselect_all &lt;- function(data, ...) {\n  \n  data |&gt;\n    select(...) |&gt;\n    head()\n  \n}\n\nselect_all(mtcars, vs, am, gear, carb)\n#&gt;                   vs am gear carb\n#&gt; Mazda RX4          0  1    4    4\n#&gt; Mazda RX4 Wag      0  1    4    4\n#&gt; Datsun 710         1  1    4    1\n#&gt; Hornet 4 Drive     1  0    3    1\n#&gt; Hornet Sportabout  0  0    3    2\n#&gt; Valiant            1  0    3    1\n\nHowever, of course, this does not just slot in to our function:\n\n\ncount_ones &lt;- function(data, ...) {\n  \n  data |&gt;\n    summarize(\n      n_ones = sum(... == 1)\n    ) |&gt;\n    pull(n_ones)\n  \n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt; Error in `summarize()`:\n#&gt; ℹ In argument: `n_ones = sum(... == 1)`.\n#&gt; Caused by error:\n#&gt; ! object 'vs' not found\n\nThe tidy approach to doing something to many columns is to use across():\n\n\n  mtcars |&gt;\n    summarize(\n      across(c(vs, am, gear, carb),\n            ~sum(.x == 1)\n    ))\n#&gt;   vs am gear carb\n#&gt; 1 14 13    0    7\n\nBut inside of a function, this fails:\n\ncount_ones &lt;- function(data, ...) {\n  \n  mtcars |&gt;\n    summarize(\n      across(...,\n            ~sum(.x == 1)\n    ))\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt; Error in `summarize()`:\n#&gt; ℹ In argument: `across(..., ~sum(.x == 1))`.\n#&gt; Caused by error in `across()`:\n#&gt; ! Can't compute column `vs`.\n#&gt; Caused by error:\n#&gt; ! object 'gear' not found\n\n\n\n\n\n\n\nNote\n\n\n\nAddendum after the fact…\nWhen this post made the rounds on social media, I was informed that the best solution is almost the above - I just shouldn’t have removed the c() around the variables. That is, the best solution to this problem is:\n\ncount_ones &lt;- function(data, ...) {\n  \n  mtcars |&gt;\n    summarize(\n      across(c(...),\n            ~sum(.x == 1)\n    ))\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt;   vs am gear carb\n#&gt; 1 14 13    0    7\n\n\n\nI surmise this is an arguments problem: across() expects a single argument, which is a vector of the column names, while the dots are passing the inputs along as four separate arguments.\nMy first instinct was to use dots_list() to smush the dots inputs into a single list object to hand to across(). But this fails for perhaps predictable reasons:\n\ncount_ones &lt;- function(data, ...) {\n  \n  args &lt;- dots_list(...)\n  \n  mtcars |&gt;\n    summarize(\n      across(c(args),\n            ~sum(.x == 1)\n    ))\n\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt; Error: object 'vs' not found\n\nYe Olde NSE strikes again: dots_list() is triggering the unquoted names to be evaluated, so vs not found.\nWell, we did just learn that quos() will get us a list of quosures, so let’s hit the dots with that:\n\ncount_ones &lt;- function(data, ...) {\n  \n  args &lt;- enquos(...)\n  \n  mtcars |&gt;\n    summarize(\n      across(args,\n            ~sum(.x == 1)\n    ))\n\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt; Error in `summarize()`:\n#&gt; ℹ In argument: `across(args, ~sum(.x == 1))`.\n#&gt; Caused by error in `across()`:\n#&gt; ! Can't select columns with `args`.\n#&gt; ✖ `args` must be numeric or character, not a &lt;quosures/list&gt; object.\n\nAlright, so across() can’t handle a list. One thing we could definitely do at this point is just move our map() approach to inside of the function:\n\ncount_ones &lt;- function(data, ...) {\n  \n  args &lt;- enquos(...)\n  \n  map(args,\n      \\(x) count_ones(mtcars, {{x}}))\n\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n\nFriends. I did not mean to put count_ones inside of itself. The above code fully crashed my R Session, with this delightful error.\n\n\n\nPlease enjoy my hilarious failure.\n\n\n\n\n\nTunneling has consequences.\n\n\nLet’s try this again.\n\n\ncount_ones &lt;- function(data, ...) {\n  \n  args &lt;- enquos(...)\n  \n  map(args,\n      \\(x) \n      mtcars |&gt;\n        summarize(\n          n_ones = sum({{x}} == 1)\n          ) |&gt;\n        pull(n_ones))\n\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt; [[1]]\n#&gt; [1] 14\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 13\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 0\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 7\n\nI’m tired and this is getting long …but I still really want to defeat the across() problem, because the ... + across() seems like an extremely handy construct.\nThere is one “free” solution, which is to just reduce our dataset to the columns we care about, and then tell across() to apply to everything():\n\ncount_ones &lt;- function(data, ...) {\n\n  mtcars |&gt;\n    select(...) |&gt;\n    summarize(\n      across(everything(),\n            ~sum(.x == 1)\n    ))\n\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt;   vs am gear carb\n#&gt; 1 14 13    0    7\n\nThis would probably be fine for every use case I can think of. But it’s not technically the same as using across() directly, because if you use across() inside mutate() it will keep all the other columns.\n\nExhibit A:\n\n  mtcars |&gt;\n    mutate(\n      across(c(vs, am, gear, carb),\n             sqrt)\n    ) |&gt;\n  head()\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am     gear     carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1 2.000000 2.000000\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1 2.000000 2.000000\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1 2.000000 1.000000\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0 1.732051 1.000000\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0 1.732051 1.414214\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0 1.732051 1.000000\n\n\n\nExhibit B:\n\n  mtcars |&gt;\n    select(vs, am, gear, carb) |&gt;\n    mutate(\n      across(everything(),\n            sqrt\n    )) |&gt;\n  head()\n#&gt;                   vs am     gear     carb\n#&gt; Mazda RX4          0  1 2.000000 2.000000\n#&gt; Mazda RX4 Wag      0  1 2.000000 2.000000\n#&gt; Datsun 710         1  1 2.000000 1.000000\n#&gt; Hornet 4 Drive     1  0 1.732051 1.000000\n#&gt; Hornet Sportabout  0  0 1.732051 1.414214\n#&gt; Valiant            1  0 1.732051 1.000000\n\nTeam, we gotta crack this so I can go to bed. Let’s take stock:\n\nWe know how to “freeze” the variable names from the dots into a list of quosures with enquos()\nWe need to find a way to pass that information as a vector object to across().\n\n\nSince this is a post about punctuation, let’s bring in the big guns: the TRIPLE BANG!!!\nThis guy !!! is one of my all time favorite tricks. It lets you turn a list of things into separate function arguments, which is called splicing.\n\nargs &lt;- quos(vs, am, gear, carb)\n\n## won't work, because it looks for the column named 'args'\nmtcars |&gt;\n  select(args) |&gt;\n  head()\n#&gt; Error in `select()`:\n#&gt; ! Can't select columns with `args`.\n#&gt; ✖ `args` must be numeric or character, not a &lt;quosures/list&gt; object.\n\n## will work, because it splices the contents of the `args` vector into separate inputs to select\nmtcars |&gt;\n  select(!!!args) |&gt;\n  head()\n#&gt;                   vs am gear carb\n#&gt; Mazda RX4          0  1    4    4\n#&gt; Mazda RX4 Wag      0  1    4    4\n#&gt; Datsun 710         1  1    4    1\n#&gt; Hornet 4 Drive     1  0    3    1\n#&gt; Hornet Sportabout  0  0    3    2\n#&gt; Valiant            1  0    3    1\n\nThe bad news: What we want here is the opposite of splicing: we want our list of quosures to become a vector of quosures.\nThe good news: If only we had a function that takes multiple arguments and concatenates them into a vector….\n\n\n\nActually, c() is for concatenate.\n\n\n\ncount_ones &lt;- function(data, ...) {\n  \n  args &lt;- enquos(...)\n  \n  data |&gt;\n    summarize(\n      across(c(!!!args),\n            ~sum(.x == 1)\n    ))\n\n}\n\ncount_ones(mtcars, vs, am, gear, carb)\n#&gt;   vs am gear carb\n#&gt; 1 14 13    0    7\n\nBoom! It still feels a little annoying to me that we had to freeze - splice - concatenate, that feels like too many steps, but I’ll take it. I can go to bed unfrustrated!\nThus ends my stream-of-consciousness journey into NSE. If you came along with me this far, thanks for hanging out, and let me know if there is any rlang trickery that I missed!"
  },
  {
    "objectID": "posts/2025-07-21-hex_stickers/index.html",
    "href": "posts/2025-07-21-hex_stickers/index.html",
    "title": "Hex Magnets!",
    "section": "",
    "text": "Today’s topic is not particularly technical, my apologies to any Serious Business RSS followers.\nBut it is undeniably R related!\nI want to take 10 minutes to share how I turned my hex sticker collection into whiteboard magnets, in case you too want to get crafty with your stickies."
  },
  {
    "objectID": "posts/2025-07-21-hex_stickers/index.html#part-one-a-quarantine-distraction",
    "href": "posts/2025-07-21-hex_stickers/index.html#part-one-a-quarantine-distraction",
    "title": "Hex Magnets!",
    "section": "Part One: A Quarantine Distraction",
    "text": "Part One: A Quarantine Distraction\nOur story begins in the year 2020. Three things happen around the same time:\n\nThe world went on lockdown, and I am bored.\nI upgrade my laptop and experience the complete and utter agony of realizing my hex stickers are stuck to the old one. (I still have said laptop, if only to preserve my vintage R Chicken Ladies sticker…)\nI get a big rolling whiteboard, for brainstorming and to serve as the background of my Zooms during remote teaching.\n\nThen the brainwave - if these stickers were magnets, I could keep them forever on my whiteboard, and also have a sweet hex sticker wall style background.\nFortunately, with the infinite downtime of quarantine, I found a few hours to spend carefully cutting hexagons out of adhesive magnetic sheets. (Seriously, this took multiple hours and my hands were unbelievably sore from the scissors after!)\nThe result:\n\n\n\nA pretty sweet setup!"
  },
  {
    "objectID": "posts/2025-07-21-hex_stickers/index.html#part-two-a-first-attempt",
    "href": "posts/2025-07-21-hex_stickers/index.html#part-two-a-first-attempt",
    "title": "Hex Magnets!",
    "section": "Part Two: A first attempt",
    "text": "Part Two: A first attempt\nFast forward to 2025, and now own a Cricut Explore Air 2, mainly for the purpose of cutting vinyl to make my Roller Derby jerseys.\nI also have an ungodly pile of hex stickers collected over the years, burning a hole in my desk drawer, that I have not found the time or motivation to magnetify. But wait! I own a cutting machine!\nAfter some research, the internet tells me that Explore Air line can cut magnets with a deep cut blade.\nReader, I bought the blade. It did not succeed.\nI tried so many settings - starting with the “0.6mm magnet sheet” preset in the Cricut Design Space, then eventually moving up to maximum possible pressure, six passes. It still couldn’t quite make it all the way through the material.\n\n\n\n\n\n\nCONCLUSION: I don’t think the Explore Air line has the pressure chops to do the thicker magnet sheets. It would probably work on the 3mm ones."
  },
  {
    "objectID": "posts/2025-07-21-hex_stickers/index.html#part-three-success",
    "href": "posts/2025-07-21-hex_stickers/index.html#part-three-success",
    "title": "Hex Magnets!",
    "section": "Part Three: Success!",
    "text": "Part Three: Success!\nSo, obviously I had to go buy a Cricut Maker. I didn’t need top-of-the-line, so I got it cheap on Facebook Marketplace, so I basically saved money right?\n\n\n\nThe math checks out.\n\n\nAnyways, I also had to get a special Knife Blade, per the interwebs.\nAnd…. this worked!\n  \nHere is the current state of the whiteboard, which is about half (!) the stickers from my pile:\n\n\n\nI like how it looks like continents and islands.\n\n\nP.S. My hex sticker wishlist for this summer’s conferences is: more holographics, dbplyr, dtplyr, ellmer, httr2, tune, workflows, and my white whale: evil cupcake version of recipes!\nAnd if you want something more permanent than even magnets?\nWell, you can always just do what I did and put the hex directly on your body…\n\n\n\nBonus points if you can guess how I picked the colors."
  },
  {
    "objectID": "posts/2025-07-21-hex_stickers/index.html#appendix-crafting-nerd-info",
    "href": "posts/2025-07-21-hex_stickers/index.html#appendix-crafting-nerd-info",
    "title": "Hex Magnets!",
    "section": "Appendix: Crafting Nerd Info:",
    "text": "Appendix: Crafting Nerd Info:\n\n\n\n\n\n\nSpecs that worked for me:\nCricut Maker 1\nKnife Blade: “Tooling Leather” preset\nStop the cut manually after 3 passes.\n(Unfortunately, Cricut doesn’t let you do custom settings with the Knife Blade, so I couldn’t just set it to fewer passes.)\nClick here to download the svg I uploaded to the Cricut software for the cut. I actually used ChatGPT to make this for me! Make sure to click and drag a little bit in the Cricut studio to be sure the hexagons are 2 inches tip to tip.\n\n\n\n\nFAQ\n\nCould you not just use the Deep Cut blade on the Maker, which does higher pressure than the Explore Air 2?\n\nTried that before buying the Knife Blade. It did not get through the material.\n\nWhy didn’t you just use the 3mm sheets instead before buying a new machine?\n\nI already had made the original first set with 6mm sheets and it would drive me crazy to have them be different thicknesses.\nAlso, I will use the machine for other crafty things too.\n\nWhat kind of magnet sheets do you recommend?\n\nI don’t think it matters, I just snagged some at Michael’s.\nI opted for the adhesive kind, so that if I want to re-use the stickers for something else for some reason, I can still take them off. But of course, you could use the non-adhesive kind and just stick your stickers to it.\n\nWhy didn’t you get a Maker 3 or 4?\n\nNo need. The only thing those buy you is faster cutting; they don’t have better abilities, and I don’t care that much about speed.\n\nWhat about [other cutting machine]?\n\nI’m an amateur and had only heard of Cricut so I bought one used on Facebook, I didn’t really research other options. I’m sure some would work.\n(Don’t use a laser cutter though - the fumes from laser cutting magnet are hazardous!)\n\nWill you cut some for me?\n\nSorry but no, I’m not interested in the Etsy life. Plus the magnet material wears out my knife blade super fast."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "paRticles",
    "section": "",
    "text": "UseR!2025: My Top 10\n\n\n\n\n\n\nconf\n\n\n\n\n\n\n\n\n\nAug 16, 2025\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nSpeed Testing: Three Levels\n\n\n\n\n\n\nprogramming\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nJul 23, 2025\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nHex Magnets!\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\nJul 21, 2025\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nPunctuation and Other Problems\n\n\n\n\n\n\nrandom\n\n\nnse\n\n\n\n\n\n\n\n\n\nApr 24, 2025\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nMove your aes\n\n\n\n\n\n\nrandom\n\n\nopinion\n\n\nteaching\n\n\n\n\n\n\n\n\n\nNov 22, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nLook at your objects\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nNotes from Posit::conf session on Quarto\n\n\n\n\n\n\nconf\n\n\n\n\n\n\n\n\n\nAug 14, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\nFirst thoughts on Positron\n\n\n\n\n\n\nnews\n\n\nopinion\n\n\n\n\n\n\n\n\n\nAug 5, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\n1-D Illustration of LDA vs QDA with {distributional}\n\n\n\n\n\n\nteaching\n\n\nnew packages\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\n\n\n\n\n\n\npaRticles\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 13, 2024\n\n\nKelly Bodwin\n\n\n\n\n\n\nNo matching items"
  }
]